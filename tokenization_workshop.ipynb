{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import networkx as nx\n",
    "import graphviz\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-dark')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b8f71aa0ee449581a7b63ea091a98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='Enter your text here...', description='Text:', layout=Layout(height='100…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_tokenization(text)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_tokenize_custom(text):\n",
    "    \"\"\"Custom word tokenization implementation\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on whitespace\n",
    "    words = text.split()\n",
    "    # Remove punctuation\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "    # Remove empty strings\n",
    "    words = [word for word in words if word]\n",
    "    return words\n",
    "\n",
    "def sentence_tokenize_custom(text):\n",
    "    \"\"\"Custom sentence tokenization implementation\"\"\"\n",
    "    # Split on sentence boundaries\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Clean up whitespace\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    # Remove empty sentences\n",
    "    sentences = [s for s in sentences if s]\n",
    "    return sentences\n",
    "\n",
    "# Interactive widget for custom text input\n",
    "text_input = widgets.Textarea(\n",
    "    value='Enter your text here...',\n",
    "    placeholder='Type something',\n",
    "    description='Text:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '80%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "def update_tokenization(text):\n",
    "    print(\"Word Tokens:\")\n",
    "    print(word_tokenize_custom(text))\n",
    "    print(\"\\nSentence Tokens:\")\n",
    "    print(sentence_tokenize_custom(text))\n",
    "\n",
    "interact(update_tokenization, text=text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Regular Expressions (Regex) in Tokenization\n",
    "\n",
    "Regular expressions are powerful patterns used to match and manipulate text. Let's break down the regex patterns we use in tokenization with interactive examples.\n",
    "\n",
    "### Common Regex Patterns in Tokenization\n",
    "\n",
    "1. `\\w+` - Matches one or more word characters (letters, numbers, underscore)\n",
    "2. `[.!?]+` - Matches one or more sentence-ending punctuation marks\n",
    "3. `[^\\w\\s]` - Matches any character that is NOT a word character or whitespace\n",
    "4. `\\s+` - Matches one or more whitespace characters\n",
    "\n",
    "Try these patterns in the interactive tool above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07111a01095442a7ac8413a9e5790241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='\\\\w+', description='Pattern:', layout=Layout(width='80%'), style=TextStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_regex_explanation(pattern, text)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add this as a new cell\n",
    "def explain_regex(pattern, text):\n",
    "    \"\"\"Interactive regex explanation tool\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Find all matches\n",
    "    matches = re.finditer(pattern, text)\n",
    "    \n",
    "    # Create a colored version of the text\n",
    "    colored_text = text\n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"Pattern: {pattern}\\n\")\n",
    "    print(\"Matches found:\")\n",
    "    \n",
    "    for match in matches:\n",
    "        start, end = match.span()\n",
    "        print(f\"\\nMatch: '{match.group()}' at positions {start}-{end}\")\n",
    "        \n",
    "        # Add color to matched text\n",
    "        colored_text = colored_text[:start + offset] + \\\n",
    "                      '\\033[92m' + match.group() + '\\033[0m' + \\\n",
    "                      colored_text[end + offset:]\n",
    "        offset += 11  # Length of color codes\n",
    "    \n",
    "    print(f\"\\nText with matches highlighted:\")\n",
    "    print(colored_text)\n",
    "\n",
    "# Interactive widget for regex testing\n",
    "pattern_input = widgets.Text(\n",
    "    value=r'\\w+',\n",
    "    description='Pattern:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '80%'}\n",
    ")\n",
    "\n",
    "text_input = widgets.Textarea(\n",
    "    value='Enter text to test regex pattern...',\n",
    "    placeholder='Type something',\n",
    "    description='Text:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '80%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "def update_regex_explanation(pattern, text):\n",
    "    explain_regex(pattern, text)\n",
    "\n",
    "interact(update_regex_explanation, pattern=pattern_input, text=text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tokenization with NLP Libraries\n",
    "\n",
    "Now let's explore tokenization using popular NLP libraries. We'll compare different tokenization methods and see how they handle various types of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167ff03d662d48cc9492f62bf5673df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='Enter text to compare different tokenizers...', description='Text:', lay…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare_tokenizers(text)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize tokenizers\n",
    "nltk_tokenizer = nltk.tokenize.word_tokenize\n",
    "spacy_tokenizer = nlp.tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def compare_tokenizers(text):\n",
    "    \"\"\"Compare different tokenization methods\"\"\"\n",
    "    print(f\"Original text: {text}\\n\")\n",
    "    \n",
    "    print(\"NLTK Tokenization:\")\n",
    "    print(nltk_tokenizer(text))\n",
    "    print()\n",
    "    \n",
    "    print(\"spaCy Tokenization:\")\n",
    "    print([token.text for token in spacy_tokenizer(text)])\n",
    "    print()\n",
    "    \n",
    "    print(\"BERT Tokenization:\")\n",
    "    print(bert_tokenizer.tokenize(text))\n",
    "\n",
    "# Interactive widget for tokenizer comparison\n",
    "text_input = widgets.Textarea(\n",
    "    value='Enter text to compare different tokenizers...',\n",
    "    placeholder='Type something',\n",
    "    description='Text:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '80%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "interact(compare_tokenizers, text=text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/samarmohanty/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualizations\n",
    "\n",
    "Let's create visual representations of the tokenization process. We'll show word frequency distributions and the tokenization flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e5c147f8974c4c8b53e98b0d9253fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='Enter text for visualization...', description='Text:', layout=Layout(hei…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_tokenization(text)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def visualize_tokenization(text):\n",
    "    \"\"\"Create interactive visualization of tokenization process\"\"\"\n",
    "    # Create word frequency plot\n",
    "    words = word_tokenize_custom(text)\n",
    "    word_freq = pd.Series(words).value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=word_freq.values[:20], y=word_freq.index[:20])\n",
    "    plt.title('Top 20 Most Frequent Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create tokenization flow diagram\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from([\n",
    "        ('Raw Text', 'Preprocessed'),\n",
    "        ('Preprocessed', 'Tokenized'),\n",
    "        ('Tokenized', 'Final Output')\n",
    "    ])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    nx.draw(G, with_labels=True, node_color='lightblue', \n",
    "            node_size=2000, arrowsize=20)\n",
    "    plt.title('Tokenization Process Flow')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget for visualization\n",
    "text_input = widgets.Textarea(\n",
    "    value='Enter text for visualization...',\n",
    "    placeholder='Type something',\n",
    "    description='Text:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '80%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "interact(visualize_tokenization, text=text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges and Edge Cases\n",
    "\n",
    "Let's explore some common challenges in tokenization and how to handle them. We'll look at contractions, special characters, and other edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc31e1094f314ab7b18ee73a1de35ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value=\"I don't like this! It's too expensive...\", description='Text:', layout=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.explore_challenges(text)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explore_challenges(text):\n",
    "    \"\"\"Demonstrate tokenization challenges and solutions\"\"\"\n",
    "    # Handle contractions\n",
    "    contractions = {\n",
    "        \"n't\": \" not\",\n",
    "        \"'m\": \" am\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\"\n",
    "    }\n",
    "    \n",
    "    def expand_contractions(text):\n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        return text\n",
    "    \n",
    "    # Handle special characters\n",
    "    def handle_special_chars(text):\n",
    "        # Replace special characters with spaces\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nAfter handling contractions:\")\n",
    "    print(expand_contractions(text))\n",
    "    print(\"\\nAfter handling special characters:\")\n",
    "    print(handle_special_chars(text))\n",
    "\n",
    "# Interactive widget for challenges\n",
    "text_input = widgets.Textarea(\n",
    "    value=\"I don't like this! It's too expensive...\",\n",
    "    placeholder='Type something with contractions and special characters',\n",
    "    description='Text:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout={'width': '80%', 'height': '100px'}\n",
    ")\n",
    "\n",
    "interact(explore_challenges, text=text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Further Resources\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Tokenization is a crucial first step in NLP pipelines\n",
    "2. Different tokenization methods serve different purposes\n",
    "3. Understanding edge cases is important for robust tokenization\n",
    "4. Regular expressions are powerful tools for text manipulation\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [spaCy Documentation](https://spacy.io/)\n",
    "- [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "- [Natural Language Processing with Python](https://www.nltk.org/book/)\n",
    "- [Regex Tutorial](https://www.regular-expressions.info/tutorial.html)\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "1. Try tokenizing different types of financial documents\n",
    "2. Experiment with different tokenization parameters\n",
    "3. Build your own custom tokenizer for specific use cases\n",
    "4. Practice writing regex patterns for different text patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
